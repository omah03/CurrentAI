{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wirkHgIqdy0_"
   },
   "source": [
    "# Welcome to assignment #4!\n",
    "\n",
    "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry. We need you to upload the .ipynb-file and the exported .pdf of this notebook.\n",
    "\n",
    "If you have any questions, ask them in either in the tutorials or in the \"Mattermost\" channel. The channel is called SSL_WS_2324, you can join the server using this [Link](https://mattermost.imp.fu-berlin.de/signup_user_complete/?id=h5ssupqokprtpyf4dr7xabiwpc&md=link&sbr=su) and can search for the public channel.\n",
    "\n",
    "\n",
    "This week is all about attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNBkNS04dy1C"
   },
   "source": [
    "# Slide Review\n",
    "\n",
    "[Google Form](https://forms.gle/u2BeWjDEQ5ZW1jZk8) for the slide review. Please take a minute to scroll over the slides again and improve your lecture.\n",
    "\n",
    "Please make sure to only choose your top 5 slides per lecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecS4eUTldy1D"
   },
   "source": [
    "# PapagAI\n",
    "\n",
    "From the second week onwards we started the reflective study.\n",
    "Register on the [PapagAI website](https://www.papag.ai) and write your first reflection about your impressions and challenges in the context of the lectures and tutorials you had this and previous week. The size of reflection can be anywhere bigger than 100 words. You can check out this [YouTube video](https://www.youtube.com/watch?v=QdmZHocZQBk&ab_channel=FernandoRamosL%C3%B3pez) with instructions on how to register, create a reflection and get an ai feedback.\n",
    "\n",
    "Please note, that this task is an obligatory one for this course and make sure each of you does the reflection, not only one person per group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KO3wnqZ5dy1E"
   },
   "source": [
    "#### Please state both names of your group members here:\n",
    "Authors: Omar Ahmed and Can Aydin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAkA1HWWdy1E"
   },
   "source": [
    "# Assignment 4: Transformers\n",
    "\n",
    "## Ex. 4.1 Attention\n",
    "\n",
    "Build the self-attension layer by yourself like it is introduced in [Attention is all you need](https://arxiv.org/abs/1706.03762). Make sure to combine Query, Key and Value in the right way and to apply the softmax function to the attention scores. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBc-zIg-dy1F"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVaBURz0FvPC"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9K7cK5Gdy1G"
   },
   "outputs": [],
   "source": [
    "class multiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, heads, embedding_dimension, attention_vectors_dimension):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.attention_vector_dimension = attention_vectors_dimension\n",
    "\n",
    "        # assert that the the attention vector dimension is always divisible by the number of heads\n",
    "        assert self.attention_vector_dimension % self.heads == 0\n",
    "\n",
    "        # create one weight matrices for queries, keys, values for one attention head\n",
    "        # If dimension of attention vector = 64 and number of attention heads = 8 => 64 * 8 = 512\n",
    "        # then the dimensions of the weight matrices are 512 x 512, if embedding dimensions are 512.\n",
    "        # The weight matrices are splitted into the number of heads to perfom dot product and then concatenated again\n",
    "        self.W_q = nn.Linear(self.embedding_dimension, self.heads * self.attention_vector_dimension, bias=False)\n",
    "        self.W_k = nn.Linear(self.embedding_dimension, self.heads * self.attention_vector_dimension, bias=False)\n",
    "        self.W_v = nn.Linear(self.embedding_dimension, self.heads * self.attention_vector_dimension, bias=False)\n",
    "\n",
    "        # create weight matrix that turns all concatenated attention heads into one output\n",
    "        # example: 64 * 8 = 512 => 512 x 512\n",
    "        self.W_o = nn.Linear(self.heads * self.attention_vector_dimension, self.embedding_dimension, bias=False)\n",
    "\n",
    "    def attention(self, Q, K, V, mask=None):\n",
    "        Q_K = torch.matmul(Q, K.transpose(-2,-1)) # (32 x 8 x seq_len x 64) @ (32 x 8 x 64 x seq_len) = (32 x 8 x seq_len x seq_len) transpose K at the the second to last index position and the last index position\n",
    "        Q_K /= math.sqrt(self.attention_vector_dimension) # (32 x 8 x seq_len x seq_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            Q_K = Q_K.masked_fill(mask==0,-1e9)\n",
    "\n",
    "        softmax_Q_K = torch.softmax(Q_K, dim=-1) # (32 x 8 x seq_len x seq_len)\n",
    "        output = torch.matmul(softmax_Q_K, V) # (32 x 8 x seq_len x seq_len) @ (32 x 8 x seq_len x 64) = (32 x 8 x seq_len x 64)\n",
    "        return output\n",
    "\n",
    "    def forward(self, X_Q, X_K, X_V, mask=None):\n",
    "        batch_size = X_Q.size(0)\n",
    "        # after the matmul of the embedded input vector and the weight matrix you have the dimension (32 x seq_len x 512), where seq_len the\n",
    "        # number of words in the sequence is. Now, to calculate the attention score, you need to reshape the weight matrix to dimensions (32 x seq_len x 8 x 64),\n",
    "        # which is just a deconcatennating the weight matrices of all attention heads.\n",
    "        # Finally, you just transpose the weight matrices at dimenions 1 and 2 which are the dimensions seq_len and heads,\n",
    "        # since you need the following matrix dimensions to calculate the attention score, see https://jalammar.github.io/illustrated-transformer/:\n",
    "        # (32 x 8 x seq_len x 64).\n",
    "        Q = self.W_q(X_Q).view(batch_size, -1, self.heads, self.attention_vector_dimension).transpose(1, 2)\n",
    "        K = self.W_k(X_K).view(batch_size, -1, self.heads, self.attention_vector_dimension).transpose(1, 2)\n",
    "        V = self.W_v(X_V).view(batch_size, -1, self.heads, self.attention_vector_dimension).transpose(1, 2)\n",
    "\n",
    "        attention_heads_scores = self.attention(Q, K, V, mask) # (32 x 8 x seq_len x 64)\n",
    "\n",
    "        # concatenate the separate attentions head scores\n",
    "        # that means you have to tranpose the seq_len and 8 (#heads) back, so you get: (32 x seq_len x 8 x 64).\n",
    "        # After that, revert the reshape and you get: (32 x seq_len x 512). With a matrix of this dimensions,\n",
    "        # you can use it on the weight matrix W_o, which will output single weight attention score for all heads: (32 x seq_len x 512)\n",
    "        # using contigous because the matrix value location in memory gets fucked up for some reason (strides??) after transpose is used\n",
    "        concatenated_heads = attention_heads_scores.transpose(1, 2).contiguous().view(batch_size, -1, self.embedding_dimension)\n",
    "        return concatenated_heads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-64KSkZPdy1J"
   },
   "source": [
    "## Ex. 4.2 Shakespeare\n",
    "\n",
    "For this exercise, we want you to generate shakespeare-like sentences using the transformer approach with the attention mechanism as its core.\n",
    "\n",
    "To solve this, we want you to implement the following pipeline:\n",
    "\n",
    "**Input -> Embedding -> N x (Transformer-Block) -> FC-Layer -> Softmax -> Output**\n",
    "\n",
    "The \"Transformer-Block\" should look like this:\n",
    "\n",
    "**Self-Attention -> LayerNorm -> FeedForward -> LayerNorm**\n",
    "\n",
    "The tinyShakespeare dataset is available [Here](https://www.tensorflow.org/datasets/catalog/tiny_shakespeare).\n",
    "\n",
    "You are free to utilize any Tokenizer for Encoding you want and you are allowed to use implementations/libraries for this. We recommend Byte Pair Encoding (BPE). [Link](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt). You are not allowed to use a transformer implementation to solve the whole task. We want you to build this pipeline around your own attention function. For the feed-forward part and fully connected layer, you may use the PyTorch implementation. **(RESULT)**\n",
    "\n",
    "How did you pick **N** for your best result? **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjnldQ3zdy1J"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torchtext\n",
    "embedding_dim = 512\n",
    "heads = 8\n",
    "attention_vector_dim = 64\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dimension, feedForwardDimension):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(embedding_dimension, feedForwardDimension)\n",
    "        self.output = nn.Linear(feedForwardDimension, embedding_dimension)\n",
    "\n",
    "    def forward(self, attention_scores):\n",
    "        # attention_scores has dimension: (32 x seq_len x 512)\n",
    "        x = F.relu(self.input(attention_scores))\n",
    "        output = self.output(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, heads, embedding_dim, attention_vector_dim, target_dim, N, vocab_size):\n",
    "        super(Net, self).__init_()\n",
    "        self.embedding = nn.Embedding(voacb_size, embedding_dim)\n",
    "        self.heads = heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.attention_vector_dim = attention_vector_dim\n",
    "        self.N = N\n",
    "        self.target_dim = target_dim # the dimension of the target vocab\n",
    "\n",
    "        self.transformerBlock = nn.Sequential(\n",
    "            multiheadSelfAttention(heads=heads, embedding_dimension=embedding_dim, attention_vectors_dimension=attention_vector_dim),\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            FeedForward(embedding_dimension=embedding_dim, feedForwardDimension=4096),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.transformerLayers = nn.ModuleList(copy.deepcopy(transformerBlock) for i in range(N))\n",
    "        self.fc = nn.Linear(self.embedding_dim, self.target_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        tokenizer = torchtext.get_tokenizer(\"basic_english\")\n",
    "        tokens = tokenizer(inp)\n",
    "        x = self.embedding(tokens)\n",
    "        for i in range(self.N): # NxTransformerBlock\n",
    "            x = self.transformerLayers[i](x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        x = F.softmax(x, dim= -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUWFRzEYGAhn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emAgDxi6GBGY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
